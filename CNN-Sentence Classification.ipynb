{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks for Sentence Classification\n",
    "### Yoon Kim\n",
    "http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let $x_i ∈ R^k$ be the $k-dimensional$ word vector corresponding to the $i^{th} word$ in the sentence. A sentence of length $n$ (padded where necessary) is represented as $$x_{1:n} = x_1 ⊕ x_2 ⊕ . . . ⊕ x_n$$ where $⊕$ is the concatenation operator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = open(\"data/rt-polarity.pos\", 'r').readlines() + open(\"data/rt-polarity.neg\", 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_chars(sentences, func=lambda x: x):\n",
    "    letters = Counter()\n",
    "    for sentence in sentences:\n",
    "        for ch in func(sentence):\n",
    "            if not ch.isspace():\n",
    "                letters[ch] += 1\n",
    "    return sorted(letters.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '[', ']', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\x85', '\\x91', '\\x96', '\\x97', '\\xbd', '\\xc9', '\\xda', '\\xe0', '\\xe1', '\\xe2', '\\xe3', '\\xe6', '\\xe7', '\\xe8', '\\xe9', '\\xea', '\\xed', '\\xef', '\\xf1', '\\xf3', '\\xf4', '\\xf5', '\\xf6', '\\xfa', '\\xfb', '\\xfc']\n"
     ]
    }
   ],
   "source": [
    "print(get_chars(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word should have alphanumeric and punctuation chars. Figure 1 tells us that we should treat contraction words separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_unwanted_chars(sentence):\n",
    "    sentence = re.sub(\"[^A-Za-z0-9,!?\\'()]\", \" \", sentence)\n",
    "    sentence = ' '.join(nltk.word_tokenize(sentence))\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', \"'\", '(', ')', ',', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print(get_chars(sentences, func=remove_unwanted_chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the max length of all the sentences and add padding for sentences with length less than max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_sentences = map(lambda x: x.strip(), open(\"data/rt-polarity.pos\", \"rb\").readlines())\n",
    "negative_sentences = map(lambda x: x.strip(), open(\"data/rt-polarity.neg\", \"rb\").readlines())\n",
    "positive_labels = [[0, 1]] * len(positive_sentences)\n",
    "negative_labels = [[1, 0]] * len(negative_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = map(lambda x: remove_unwanted_chars(x).split(), positive_sentences + negative_sentences)\n",
    "y = np.array(positive_labels + negative_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_length = max(len(x) for x in X)\n",
    "X_padded = []\n",
    "for x in X:\n",
    "    length = len(x)\n",
    "    padding = max_length - length\n",
    "    x = x + [\"<NULL>\"] * padding\n",
    "    X_padded.append(x)\n",
    "    \n",
    "X = X_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We use the publicly available word2vec vectors that were trained on 100 billion words from Google\n",
    "News. The vectors have dimensionality of 300 and were trained using the continuous bag-of-words architecture (Mikolov et al., 2013). Words not present in the set of pre-trained words are initialized randomly.\n",
    "\n",
    "> **CNN-rand**: Our baseline model where all words are randomly initialized and then modified during training.\n",
    "\n",
    "> **CNN-staitc**: A model with pre-trained vectors from word2vec. All words—including the unknown ones that are randomly initialized—are kept static and only the other parameters of the model are learned.\n",
    "\n",
    "> When randomly initializing words not in word2vec, we obtained slight improvements by sampling each dimension from\n",
    "$U [−a, a]$ where $a$ was chosen such that the randomly initialized vectors have the same variance as the pre-trained ones.\n",
    "\n",
    "Things to do:\n",
    "1. Create a vocabulary from all the sentences\n",
    "2. Create word vectors using word2vec (use pre-trained googlenews-vectors-negative300.bin)\n",
    "3. Find variance of word in vocab present in word2vec and create randomly initialized vectors for words not present in word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = Counter(itertools.chain(*X)).keys()\n",
    "vocabulary_map = {word: i for i, word in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_word_vectors(vocab):\n",
    "    word_vecs = {}\n",
    "    from gensim.models import Word2Vec\n",
    "    model = Word2Vec.load_word2vec_format(\"data/GoogleNews-vectors-negative300.bin\")\n",
    "    for word in vocab:\n",
    "        if word in model:\n",
    "            word_vecs[word] = model[word]\n",
    "    return word_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordvectors = get_word_vectors(vocabulary_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 10662\n",
      "V: 18588\n",
      "V_pre: 16461\n",
      "std: 0.177474\n"
     ]
    }
   ],
   "source": [
    "std = np.std(np.array(wordvectors.values()))\n",
    "print(\"N: \" + str(len(X)))\n",
    "print(\"V: \" + str(len(vocabulary)))\n",
    "print(\"V_pre: \" + str(len(wordvectors)))\n",
    "print(\"std: \" + str(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_matrix(sentences, vocab, wordvector, std):\n",
    "    dataset_size = len(sentences)\n",
    "    sentence_length = len(sentences[0])\n",
    "    k = 300\n",
    "    matrix = np.zeros(shape=(dataset_size, sentence_length, k), dtype=np.float32)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, word in enumerate(sentence):\n",
    "            if word == '<NULL>':\n",
    "                matrix[i][j] = np.zeros(k, dtype=np.float32)\n",
    "            if word in wordvector:\n",
    "                matrix[i][j] = wordvector[word]\n",
    "            else:\n",
    "                matrix[i][j] = np.random.uniform(std, -std, k)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "> For all datasets we use: rectified linear units, filter\n",
    "windows $(h)$ of $3, 4, 5$ with $100$ feature maps each,\n",
    "dropout rate $(p)$ of $0.5$, $l_2$ constraint $(s)$ of $3$, and\n",
    "mini-batch size of $50$. \n",
    "\n",
    "\n",
    "\n",
    "> Training is done through stochastic gradient descent over shuffled mini-batches with the\n",
    "Adadelta update rule (Zeiler, 2012).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relu = tf.nn.relu\n",
    "h = [3, 4, 5]\n",
    "p = 0.5\n",
    "s = 3\n",
    "batch_size = 50\n",
    "Adadelta = tf.train.AdadeltaOptimizer\n",
    "feature_map = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: CNN-static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = len(X[0])\n",
    "k = 300\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_x = tf.placeholder(tf.float32, shape=(None, n, k))\n",
    "input_y = tf.placeholder(tf.float32, shape=(None, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expand_input_x = tf.expand_dims(input_x, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layer with filter window size 3, 4, 5 and then max-over-time pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers = []\n",
    "for filter_size in h:\n",
    "    filter_shape = [filter_size, k, 1, feature_map]\n",
    "    W = weight_variable(filter_shape)\n",
    "    b = bias_variable([feature_map])\n",
    "    conv = relu(tf.nn.bias_add(tf.nn.conv2d(expand_input_x, W, strides=[1, 1, 1, 1], padding=\"VALID\"),\n",
    "                          b))\n",
    "    max_pool = tf.nn.max_pool(conv, ksize=[1, n-filter_size+1, 1, 1], strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "    layers.append(max_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_feature_maps = len(h) * feature_map\n",
    "conv_concat = tf.reshape(tf.concat(3, layers), (-1, total_feature_maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_layer = tf.nn.dropout(conv_concat, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = weight_variable((total_feature_maps, n_classes))\n",
    "b = bias_variable([n_classes])\n",
    "fc_layer = tf.nn.xw_plus_b(dropout_layer, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = tf.argmax(fc_layer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(fc_layer, input_y))\n",
    "train_step = Adadelta(learning_rate=1e-3, rho=0.95, epsilon=1e-6).minimize(cross_entropy)\n",
    "correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Test:* Test set size (CV means there was no standard train/test split and thus 10-fold CV was used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_train_test_split(dataset, i, k_fold):\n",
    "    train_x, train_y, test_x, test_y = [], [], [], []\n",
    "    fold_size = len(dataset) / k_fold\n",
    "    for j in range(k_fold):\n",
    "        if j == i:\n",
    "            test_x.extend(build_matrix(dataset[j*fold_size:(j+1)*fold_size], vocabulary_map, wordvectors, std))\n",
    "            test_y.extend(y[j*fold_size:(j+1)*fold_size])\n",
    "        else:\n",
    "            train_x.extend(build_matrix(dataset[j*fold_size:(j+1)*fold_size], vocabulary_map, wordvectors, std))\n",
    "            train_y.extend(y[j*fold_size:(j+1)*fold_size])\n",
    "    train_x.extend(build_matrix(dataset[(j+1)*fold_size:], vocabulary_map, wordvectors, std))\n",
    "    train_y.extend(y[(j+1)*fold_size:])\n",
    "    return {\n",
    "        \"train\": {\n",
    "            \"data\": np.array(train_x),\n",
    "            \"label\": np.array(train_y)\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"data\": np.array(test_x),\n",
    "            \"label\": np.array(test_y)\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(dataset, batch_size):\n",
    "    iterations = int(ceil(len(dataset[\"data\"])/batch_size))\n",
    "    for i in range(iterations):\n",
    "        batch = {\n",
    "            \"data\": dataset[\"data\"][i*batch_size:(i+1)*batch_size],\n",
    "            \"label\": dataset[\"label\"][i*batch_size:(i+1)*batch_size]\n",
    "        }\n",
    "        yield i, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded!\n",
      "Training epoch 1\n",
      "  Epoch: [1][0/191] Loss 0.43 Accuracy 0.72\n",
      "  Epoch: [1][1/191] Loss 0.38 Accuracy 0.76\n",
      "  Epoch: [1][2/191] Loss 0.40 Accuracy 0.76\n",
      "  Epoch: [1][3/191] Loss 0.23 Accuracy 0.84\n",
      "  Epoch: [1][4/191] Loss 0.48 Accuracy 0.70\n",
      "  Epoch: [1][5/191] Loss 0.38 Accuracy 0.82\n",
      "  Epoch: [1][6/191] Loss 0.44 Accuracy 0.74\n",
      "  Epoch: [1][7/191] Loss 0.41 Accuracy 0.86\n",
      "  Epoch: [1][8/191] Loss 0.52 Accuracy 0.78\n",
      "  Epoch: [1][9/191] Loss 0.35 Accuracy 0.90\n",
      "  Epoch: [1][10/191] Loss 0.41 Accuracy 0.66\n",
      "  Epoch: [1][11/191] Loss 0.47 Accuracy 0.80\n",
      "  Epoch: [1][12/191] Loss 0.60 Accuracy 0.86\n",
      "  Epoch: [1][13/191] Loss 0.35 Accuracy 0.76\n",
      "  Epoch: [1][14/191] Loss 0.32 Accuracy 0.88\n",
      "  Epoch: [1][15/191] Loss 0.43 Accuracy 0.86\n",
      "  Epoch: [1][16/191] Loss 0.49 Accuracy 0.76\n",
      "  Epoch: [1][17/191] Loss 0.52 Accuracy 0.78\n",
      "  Epoch: [1][18/191] Loss 0.46 Accuracy 0.80\n",
      "  Epoch: [1][19/191] Loss 0.32 Accuracy 0.82\n",
      "  Epoch: [1][20/191] Loss 0.48 Accuracy 0.82\n",
      "  Epoch: [1][21/191] Loss 0.36 Accuracy 0.90\n",
      "  Epoch: [1][22/191] Loss 0.32 Accuracy 0.74\n",
      "  Epoch: [1][23/191] Loss 0.44 Accuracy 0.76\n",
      "  Epoch: [1][24/191] Loss 0.35 Accuracy 0.76\n",
      "  Epoch: [1][25/191] Loss 0.43 Accuracy 0.76\n",
      "  Epoch: [1][26/191] Loss 0.28 Accuracy 0.72\n",
      "  Epoch: [1][27/191] Loss 0.43 Accuracy 0.80\n",
      "  Epoch: [1][28/191] Loss 0.35 Accuracy 0.82\n",
      "  Epoch: [1][29/191] Loss 0.35 Accuracy 0.78\n",
      "  Epoch: [1][30/191] Loss 0.53 Accuracy 0.86\n",
      "  Epoch: [1][31/191] Loss 0.46 Accuracy 0.86\n",
      "  Epoch: [1][32/191] Loss 0.32 Accuracy 0.82\n",
      "  Epoch: [1][33/191] Loss 0.45 Accuracy 0.76\n",
      "  Epoch: [1][34/191] Loss 0.24 Accuracy 0.74\n",
      "  Epoch: [1][35/191] Loss 0.44 Accuracy 0.86\n",
      "  Epoch: [1][36/191] Loss 0.47 Accuracy 0.84\n",
      "  Epoch: [1][37/191] Loss 0.31 Accuracy 0.80\n",
      "  Epoch: [1][38/191] Loss 0.35 Accuracy 0.68\n",
      "  Epoch: [1][39/191] Loss 0.52 Accuracy 0.80\n",
      "  Epoch: [1][40/191] Loss 0.38 Accuracy 0.72\n",
      "  Epoch: [1][41/191] Loss 0.38 Accuracy 0.80\n",
      "  Epoch: [1][42/191] Loss 0.38 Accuracy 0.76\n",
      "  Epoch: [1][43/191] Loss 0.49 Accuracy 0.76\n",
      "  Epoch: [1][44/191] Loss 0.34 Accuracy 0.86\n",
      "  Epoch: [1][45/191] Loss 0.48 Accuracy 0.84\n",
      "  Epoch: [1][46/191] Loss 0.45 Accuracy 0.86\n",
      "  Epoch: [1][47/191] Loss 0.40 Accuracy 0.88\n",
      "  Epoch: [1][48/191] Loss 0.31 Accuracy 0.80\n",
      "  Epoch: [1][49/191] Loss 0.36 Accuracy 0.88\n",
      "  Epoch: [1][50/191] Loss 0.51 Accuracy 0.82\n",
      "  Epoch: [1][51/191] Loss 0.31 Accuracy 0.80\n",
      "  Epoch: [1][52/191] Loss 0.32 Accuracy 0.90\n",
      "  Epoch: [1][53/191] Loss 0.39 Accuracy 0.86\n",
      "  Epoch: [1][54/191] Loss 0.36 Accuracy 0.84\n",
      "  Epoch: [1][55/191] Loss 0.41 Accuracy 0.90\n",
      "  Epoch: [1][56/191] Loss 0.41 Accuracy 0.82\n",
      "  Epoch: [1][57/191] Loss 0.41 Accuracy 0.78\n",
      "  Epoch: [1][58/191] Loss 0.49 Accuracy 0.82\n",
      "  Epoch: [1][59/191] Loss 0.46 Accuracy 0.88\n",
      "  Epoch: [1][60/191] Loss 0.50 Accuracy 0.80\n",
      "  Epoch: [1][61/191] Loss 0.32 Accuracy 0.80\n",
      "  Epoch: [1][62/191] Loss 0.47 Accuracy 0.86\n",
      "  Epoch: [1][63/191] Loss 0.59 Accuracy 0.90\n",
      "  Epoch: [1][64/191] Loss 0.34 Accuracy 0.82\n",
      "  Epoch: [1][65/191] Loss 0.34 Accuracy 0.86\n",
      "  Epoch: [1][66/191] Loss 0.43 Accuracy 0.82\n",
      "  Epoch: [1][67/191] Loss 0.34 Accuracy 0.78\n",
      "  Epoch: [1][68/191] Loss 0.63 Accuracy 0.88\n",
      "  Epoch: [1][69/191] Loss 0.50 Accuracy 0.74\n",
      "  Epoch: [1][70/191] Loss 0.38 Accuracy 0.84\n",
      "  Epoch: [1][71/191] Loss 0.43 Accuracy 0.88\n",
      "  Epoch: [1][72/191] Loss 0.30 Accuracy 0.78\n",
      "  Epoch: [1][73/191] Loss 0.44 Accuracy 0.72\n",
      "  Epoch: [1][74/191] Loss 0.33 Accuracy 0.80\n",
      "  Epoch: [1][75/191] Loss 0.40 Accuracy 0.80\n",
      "  Epoch: [1][76/191] Loss 0.37 Accuracy 0.78\n",
      "  Epoch: [1][77/191] Loss 0.41 Accuracy 0.88\n",
      "  Epoch: [1][78/191] Loss 0.47 Accuracy 0.76\n",
      "  Epoch: [1][79/191] Loss 0.23 Accuracy 0.72\n",
      "  Epoch: [1][80/191] Loss 0.44 Accuracy 0.80\n",
      "  Epoch: [1][81/191] Loss 0.40 Accuracy 0.80\n",
      "  Epoch: [1][82/191] Loss 0.33 Accuracy 0.82\n",
      "  Epoch: [1][83/191] Loss 0.47 Accuracy 0.88\n",
      "  Epoch: [1][84/191] Loss 0.46 Accuracy 0.80\n",
      "  Epoch: [1][85/191] Loss 1.71 Accuracy 0.40\n",
      "  Epoch: [1][86/191] Loss 2.15 Accuracy 0.16\n",
      "  Epoch: [1][87/191] Loss 2.56 Accuracy 0.16\n",
      "  Epoch: [1][88/191] Loss 2.14 Accuracy 0.18\n",
      "  Epoch: [1][89/191] Loss 2.07 Accuracy 0.26\n",
      "  Epoch: [1][90/191] Loss 2.35 Accuracy 0.16\n",
      "  Epoch: [1][91/191] Loss 2.20 Accuracy 0.18\n",
      "  Epoch: [1][92/191] Loss 2.46 Accuracy 0.24\n",
      "  Epoch: [1][93/191] Loss 1.99 Accuracy 0.20\n",
      "  Epoch: [1][94/191] Loss 1.95 Accuracy 0.22\n",
      "  Epoch: [1][95/191] Loss 2.40 Accuracy 0.30\n",
      "  Epoch: [1][96/191] Loss 2.18 Accuracy 0.20\n",
      "  Epoch: [1][97/191] Loss 2.38 Accuracy 0.12\n",
      "  Epoch: [1][98/191] Loss 2.65 Accuracy 0.14\n",
      "  Epoch: [1][99/191] Loss 2.08 Accuracy 0.12\n",
      "  Epoch: [1][100/191] Loss 2.20 Accuracy 0.06\n",
      "  Epoch: [1][101/191] Loss 2.54 Accuracy 0.22\n",
      "  Epoch: [1][102/191] Loss 2.26 Accuracy 0.16\n",
      "  Epoch: [1][103/191] Loss 2.19 Accuracy 0.22\n",
      "  Epoch: [1][104/191] Loss 2.07 Accuracy 0.14\n",
      "  Epoch: [1][105/191] Loss 2.49 Accuracy 0.26\n",
      "  Epoch: [1][106/191] Loss 1.84 Accuracy 0.16\n",
      "  Epoch: [1][107/191] Loss 2.13 Accuracy 0.16\n",
      "  Epoch: [1][108/191] Loss 2.20 Accuracy 0.12\n",
      "  Epoch: [1][109/191] Loss 2.56 Accuracy 0.18\n",
      "  Epoch: [1][110/191] Loss 2.15 Accuracy 0.28\n",
      "  Epoch: [1][111/191] Loss 2.19 Accuracy 0.10\n",
      "  Epoch: [1][112/191] Loss 2.19 Accuracy 0.18\n",
      "  Epoch: [1][113/191] Loss 2.38 Accuracy 0.08\n",
      "  Epoch: [1][114/191] Loss 2.26 Accuracy 0.20\n",
      "  Epoch: [1][115/191] Loss 2.22 Accuracy 0.14\n",
      "  Epoch: [1][116/191] Loss 2.15 Accuracy 0.10\n",
      "  Epoch: [1][117/191] Loss 2.43 Accuracy 0.20\n",
      "  Epoch: [1][118/191] Loss 2.28 Accuracy 0.20\n",
      "  Epoch: [1][119/191] Loss 2.49 Accuracy 0.24\n",
      "  Epoch: [1][120/191] Loss 2.77 Accuracy 0.16\n",
      "  Epoch: [1][121/191] Loss 2.27 Accuracy 0.12\n",
      "  Epoch: [1][122/191] Loss 2.17 Accuracy 0.18\n",
      "  Epoch: [1][123/191] Loss 2.14 Accuracy 0.08\n",
      "  Epoch: [1][124/191] Loss 2.18 Accuracy 0.20\n",
      "  Epoch: [1][125/191] Loss 2.19 Accuracy 0.22\n",
      "  Epoch: [1][126/191] Loss 1.88 Accuracy 0.24\n",
      "  Epoch: [1][127/191] Loss 2.57 Accuracy 0.20\n",
      "  Epoch: [1][128/191] Loss 2.23 Accuracy 0.14\n",
      "  Epoch: [1][129/191] Loss 2.29 Accuracy 0.22\n",
      "  Epoch: [1][130/191] Loss 2.34 Accuracy 0.28\n",
      "  Epoch: [1][131/191] Loss 1.88 Accuracy 0.26\n",
      "  Epoch: [1][132/191] Loss 2.32 Accuracy 0.20\n",
      "  Epoch: [1][133/191] Loss 1.96 Accuracy 0.24\n",
      "  Epoch: [1][134/191] Loss 1.84 Accuracy 0.16\n",
      "  Epoch: [1][135/191] Loss 2.23 Accuracy 0.20\n",
      "  Epoch: [1][136/191] Loss 2.20 Accuracy 0.16\n",
      "  Epoch: [1][137/191] Loss 2.34 Accuracy 0.34\n",
      "  Epoch: [1][138/191] Loss 2.31 Accuracy 0.28\n",
      "  Epoch: [1][139/191] Loss 1.82 Accuracy 0.22\n",
      "  Epoch: [1][140/191] Loss 2.35 Accuracy 0.22\n",
      "  Epoch: [1][141/191] Loss 2.06 Accuracy 0.26\n",
      "  Epoch: [1][142/191] Loss 2.28 Accuracy 0.14\n",
      "  Epoch: [1][143/191] Loss 1.93 Accuracy 0.16\n",
      "  Epoch: [1][144/191] Loss 1.90 Accuracy 0.12\n",
      "  Epoch: [1][145/191] Loss 2.13 Accuracy 0.14\n",
      "  Epoch: [1][146/191] Loss 2.12 Accuracy 0.24\n",
      "  Epoch: [1][147/191] Loss 2.02 Accuracy 0.16\n",
      "  Epoch: [1][148/191] Loss 1.94 Accuracy 0.18\n",
      "  Epoch: [1][149/191] Loss 2.17 Accuracy 0.24\n",
      "  Epoch: [1][150/191] Loss 2.36 Accuracy 0.20\n",
      "  Epoch: [1][151/191] Loss 2.33 Accuracy 0.18\n",
      "  Epoch: [1][152/191] Loss 2.18 Accuracy 0.18\n",
      "  Epoch: [1][153/191] Loss 2.02 Accuracy 0.14\n",
      "  Epoch: [1][154/191] Loss 2.05 Accuracy 0.10\n",
      "  Epoch: [1][155/191] Loss 1.93 Accuracy 0.08\n",
      "  Epoch: [1][156/191] Loss 2.16 Accuracy 0.22\n",
      "  Epoch: [1][157/191] Loss 2.21 Accuracy 0.20\n",
      "  Epoch: [1][158/191] Loss 2.34 Accuracy 0.18\n",
      "  Epoch: [1][159/191] Loss 2.02 Accuracy 0.22\n",
      "  Epoch: [1][160/191] Loss 2.18 Accuracy 0.32\n",
      "  Epoch: [1][161/191] Loss 2.32 Accuracy 0.18\n",
      "  Epoch: [1][162/191] Loss 2.03 Accuracy 0.14\n",
      "  Epoch: [1][163/191] Loss 2.47 Accuracy 0.18\n",
      "  Epoch: [1][164/191] Loss 2.38 Accuracy 0.22\n",
      "  Epoch: [1][165/191] Loss 2.35 Accuracy 0.28\n",
      "  Epoch: [1][166/191] Loss 1.90 Accuracy 0.16\n",
      "  Epoch: [1][167/191] Loss 2.03 Accuracy 0.16\n",
      "  Epoch: [1][168/191] Loss 2.29 Accuracy 0.20\n",
      "  Epoch: [1][169/191] Loss 2.17 Accuracy 0.20\n",
      "  Epoch: [1][170/191] Loss 2.21 Accuracy 0.22\n",
      "  Epoch: [1][171/191] Loss 2.21 Accuracy 0.22\n",
      "  Epoch: [1][172/191] Loss 2.72 Accuracy 0.18\n",
      "  Epoch: [1][173/191] Loss 2.27 Accuracy 0.16\n",
      "  Epoch: [1][174/191] Loss 2.42 Accuracy 0.32\n",
      "  Epoch: [1][175/191] Loss 2.09 Accuracy 0.22\n",
      "  Epoch: [1][176/191] Loss 1.84 Accuracy 0.12\n",
      "  Epoch: [1][177/191] Loss 2.35 Accuracy 0.30\n",
      "  Epoch: [1][178/191] Loss 2.28 Accuracy 0.10\n",
      "  Epoch: [1][179/191] Loss 2.81 Accuracy 0.10\n",
      "  Epoch: [1][180/191] Loss 2.52 Accuracy 0.12\n",
      "  Epoch: [1][181/191] Loss 2.37 Accuracy 0.26\n",
      "  Epoch: [1][182/191] Loss 2.11 Accuracy 0.18\n",
      "  Epoch: [1][183/191] Loss 2.18 Accuracy 0.26\n",
      "  Epoch: [1][184/191] Loss 2.47 Accuracy 0.16\n",
      "  Epoch: [1][185/191] Loss 2.24 Accuracy 0.22\n",
      "  Epoch: [1][186/191] Loss 2.32 Accuracy 0.26\n",
      "  Epoch: [1][187/191] Loss 1.90 Accuracy 0.26\n",
      "  Epoch: [1][188/191] Loss 2.23 Accuracy 0.16\n",
      "  Epoch: [1][189/191] Loss 1.82 Accuracy 0.18\n",
      "  Epoch: [1][190/191] Loss 2.11 Accuracy 0.24\n",
      "  Epoch 1: Loss 1.41 Accuracy 0.47\n",
      "  Epoch: [1][0/21] Accuracy 0.86\n",
      "  Epoch: [1][1/21] Accuracy 0.82\n",
      "  Epoch: [1][2/21] Accuracy 0.80\n",
      "  Epoch: [1][3/21] Accuracy 0.84\n",
      "  Epoch: [1][4/21] Accuracy 0.76\n",
      "  Epoch: [1][5/21] Accuracy 0.86\n",
      "  Epoch: [1][6/21] Accuracy 0.84\n",
      "  Epoch: [1][7/21] Accuracy 0.90\n",
      "  Epoch: [1][8/21] Accuracy 0.78\n",
      "  Epoch: [1][9/21] Accuracy 0.72\n",
      "  Epoch: [1][10/21] Accuracy 0.82\n",
      "  Epoch: [1][11/21] Accuracy 0.90\n",
      "  Epoch: [1][12/21] Accuracy 0.82\n",
      "  Epoch: [1][13/21] Accuracy 0.94\n",
      "  Epoch: [1][14/21] Accuracy 0.82\n",
      "  Epoch: [1][15/21] Accuracy 0.90\n",
      "  Epoch: [1][16/21] Accuracy 0.76\n",
      "  Epoch: [1][17/21] Accuracy 0.94\n",
      "  Epoch: [1][18/21] Accuracy 0.84\n",
      "  Epoch: [1][19/21] Accuracy 0.86\n",
      "  Epoch: [1][20/21] Accuracy 0.78\n",
      "Finished Epoch 1 Accuracy 0.84\n",
      "Training epoch 2\n",
      "  Epoch: [2][0/191] Loss 0.43 Accuracy 0.74\n",
      "  Epoch: [2][1/191] Loss 0.38 Accuracy 0.80\n",
      "  Epoch: [2][2/191] Loss 0.42 Accuracy 0.90\n",
      "  Epoch: [2][3/191] Loss 0.37 Accuracy 0.86\n",
      "  Epoch: [2][4/191] Loss 0.48 Accuracy 0.70\n",
      "  Epoch: [2][5/191] Loss 0.39 Accuracy 0.82\n",
      "  Epoch: [2][6/191] Loss 0.36 Accuracy 0.86\n",
      "  Epoch: [2][7/191] Loss 0.35 Accuracy 0.84\n",
      "  Epoch: [2][8/191] Loss 0.50 Accuracy 0.88\n",
      "  Epoch: [2][9/191] Loss 0.34 Accuracy 0.72\n",
      "  Epoch: [2][10/191] Loss 0.34 Accuracy 0.82\n",
      "  Epoch: [2][11/191] Loss 0.36 Accuracy 0.74\n",
      "  Epoch: [2][12/191] Loss 0.34 Accuracy 0.82\n",
      "  Epoch: [2][13/191] Loss 0.43 Accuracy 0.92\n",
      "  Epoch: [2][14/191] Loss 0.42 Accuracy 0.82\n",
      "  Epoch: [2][15/191] Loss 0.30 Accuracy 0.70\n",
      "  Epoch: [2][16/191] Loss 0.54 Accuracy 0.76\n",
      "  Epoch: [2][17/191] Loss 0.41 Accuracy 0.86\n",
      "  Epoch: [2][18/191] Loss 0.42 Accuracy 0.94\n",
      "  Epoch: [2][19/191] Loss 0.39 Accuracy 0.82\n",
      "  Epoch: [2][20/191] Loss 0.36 Accuracy 0.84\n",
      "  Epoch: [2][21/191] Loss 0.31 Accuracy 0.78\n",
      "  Epoch: [2][22/191] Loss 0.34 Accuracy 0.74\n",
      "  Epoch: [2][23/191] Loss 0.22 Accuracy 0.86\n",
      "  Epoch: [2][24/191] Loss 0.44 Accuracy 0.80\n",
      "  Epoch: [2][25/191] Loss 0.37 Accuracy 0.84\n",
      "  Epoch: [2][26/191] Loss 0.34 Accuracy 0.90\n",
      "  Epoch: [2][27/191] Loss 0.47 Accuracy 0.74\n",
      "  Epoch: [2][28/191] Loss 0.40 Accuracy 0.78\n",
      "  Epoch: [2][29/191] Loss 0.22 Accuracy 0.84\n",
      "  Epoch: [2][30/191] Loss 0.55 Accuracy 0.84\n",
      "  Epoch: [2][31/191] Loss 0.38 Accuracy 0.84\n",
      "  Epoch: [2][32/191] Loss 0.35 Accuracy 0.88\n",
      "  Epoch: [2][33/191] Loss 0.38 Accuracy 0.74\n",
      "  Epoch: [2][34/191] Loss 0.28 Accuracy 0.88\n",
      "  Epoch: [2][35/191] Loss 0.35 Accuracy 0.72\n",
      "  Epoch: [2][36/191] Loss 0.43 Accuracy 0.86\n",
      "  Epoch: [2][37/191] Loss 0.41 Accuracy 0.88\n",
      "  Epoch: [2][38/191] Loss 0.38 Accuracy 0.82\n",
      "  Epoch: [2][39/191] Loss 0.38 Accuracy 0.80\n",
      "  Epoch: [2][40/191] Loss 0.32 Accuracy 0.92\n",
      "  Epoch: [2][41/191] Loss 0.38 Accuracy 0.78\n",
      "  Epoch: [2][42/191] Loss 0.38 Accuracy 0.84\n",
      "  Epoch: [2][43/191] Loss 0.47 Accuracy 0.90\n",
      "  Epoch: [2][44/191] Loss 0.32 Accuracy 0.88\n",
      "  Epoch: [2][45/191] Loss 0.52 Accuracy 0.88\n",
      "  Epoch: [2][46/191] Loss 0.31 Accuracy 0.86\n",
      "  Epoch: [2][47/191] Loss 0.36 Accuracy 0.80\n",
      "  Epoch: [2][48/191] Loss 0.42 Accuracy 0.78\n",
      "  Epoch: [2][49/191] Loss 0.31 Accuracy 0.80\n",
      "  Epoch: [2][50/191] Loss 0.43 Accuracy 0.90\n",
      "  Epoch: [2][51/191] Loss 0.49 Accuracy 0.80\n",
      "  Epoch: [2][52/191] Loss 0.31 Accuracy 0.88\n",
      "  Epoch: [2][53/191] Loss 0.54 Accuracy 0.72\n",
      "  Epoch: [2][54/191] Loss 0.32 Accuracy 0.80\n",
      "  Epoch: [2][55/191] Loss 0.31 Accuracy 0.82\n",
      "  Epoch: [2][56/191] Loss 0.48 Accuracy 0.84\n",
      "  Epoch: [2][57/191] Loss 0.37 Accuracy 0.78\n",
      "  Epoch: [2][58/191] Loss 0.24 Accuracy 0.88\n",
      "  Epoch: [2][59/191] Loss 0.31 Accuracy 0.84\n",
      "  Epoch: [2][60/191] Loss 0.46 Accuracy 0.88\n",
      "  Epoch: [2][61/191] Loss 0.51 Accuracy 0.86\n",
      "  Epoch: [2][62/191] Loss 0.39 Accuracy 0.84\n",
      "  Epoch: [2][63/191] Loss 0.38 Accuracy 0.88\n",
      "  Epoch: [2][64/191] Loss 0.50 Accuracy 0.68\n",
      "  Epoch: [2][65/191] Loss 0.44 Accuracy 0.80\n",
      "  Epoch: [2][66/191] Loss 0.57 Accuracy 0.80\n",
      "  Epoch: [2][67/191] Loss 0.37 Accuracy 0.82\n",
      "  Epoch: [2][68/191] Loss 0.35 Accuracy 0.78\n",
      "  Epoch: [2][69/191] Loss 0.26 Accuracy 0.78\n",
      "  Epoch: [2][70/191] Loss 0.36 Accuracy 0.84\n",
      "  Epoch: [2][71/191] Loss 0.35 Accuracy 0.82\n",
      "  Epoch: [2][72/191] Loss 0.42 Accuracy 0.84\n",
      "  Epoch: [2][73/191] Loss 0.26 Accuracy 0.80\n",
      "  Epoch: [2][74/191] Loss 0.36 Accuracy 0.78\n",
      "  Epoch: [2][75/191] Loss 0.45 Accuracy 0.84\n",
      "  Epoch: [2][76/191] Loss 0.45 Accuracy 0.72\n",
      "  Epoch: [2][77/191] Loss 0.29 Accuracy 0.76\n",
      "  Epoch: [2][78/191] Loss 0.51 Accuracy 0.88\n",
      "  Epoch: [2][79/191] Loss 0.41 Accuracy 0.74\n",
      "  Epoch: [2][80/191] Loss 0.35 Accuracy 0.84\n",
      "  Epoch: [2][81/191] Loss 0.53 Accuracy 0.70\n",
      "  Epoch: [2][82/191] Loss 0.42 Accuracy 0.76\n",
      "  Epoch: [2][83/191] Loss 0.49 Accuracy 0.82\n",
      "  Epoch: [2][84/191] Loss 0.39 Accuracy 0.80\n",
      "  Epoch: [2][85/191] Loss 1.44 Accuracy 0.38\n",
      "  Epoch: [2][86/191] Loss 2.09 Accuracy 0.30\n",
      "  Epoch: [2][87/191] Loss 2.23 Accuracy 0.18\n",
      "  Epoch: [2][88/191] Loss 2.83 Accuracy 0.14\n",
      "  Epoch: [2][89/191] Loss 1.68 Accuracy 0.18\n",
      "  Epoch: [2][90/191] Loss 2.39 Accuracy 0.18\n",
      "  Epoch: [2][91/191] Loss 2.59 Accuracy 0.18\n",
      "  Epoch: [2][92/191] Loss 1.88 Accuracy 0.26\n",
      "  Epoch: [2][93/191] Loss 2.19 Accuracy 0.14\n",
      "  Epoch: [2][94/191] Loss 2.12 Accuracy 0.24\n",
      "  Epoch: [2][95/191] Loss 2.33 Accuracy 0.16\n",
      "  Epoch: [2][96/191] Loss 2.26 Accuracy 0.22\n",
      "  Epoch: [2][97/191] Loss 2.01 Accuracy 0.20\n",
      "  Epoch: [2][98/191] Loss 2.16 Accuracy 0.10\n",
      "  Epoch: [2][99/191] Loss 2.03 Accuracy 0.26\n",
      "  Epoch: [2][100/191] Loss 2.32 Accuracy 0.18\n",
      "  Epoch: [2][101/191] Loss 2.15 Accuracy 0.08\n",
      "  Epoch: [2][102/191] Loss 2.17 Accuracy 0.16\n",
      "  Epoch: [2][103/191] Loss 2.13 Accuracy 0.14\n",
      "  Epoch: [2][104/191] Loss 2.00 Accuracy 0.18\n",
      "  Epoch: [2][105/191] Loss 2.56 Accuracy 0.12\n",
      "  Epoch: [2][106/191] Loss 2.14 Accuracy 0.26\n",
      "  Epoch: [2][107/191] Loss 2.30 Accuracy 0.22\n",
      "  Epoch: [2][108/191] Loss 1.77 Accuracy 0.16\n",
      "  Epoch: [2][109/191] Loss 1.93 Accuracy 0.14\n",
      "  Epoch: [2][110/191] Loss 2.32 Accuracy 0.10\n",
      "  Epoch: [2][111/191] Loss 2.49 Accuracy 0.22\n",
      "  Epoch: [2][112/191] Loss 2.25 Accuracy 0.16\n",
      "  Epoch: [2][113/191] Loss 2.09 Accuracy 0.10\n",
      "  Epoch: [2][114/191] Loss 1.69 Accuracy 0.14\n",
      "  Epoch: [2][115/191] Loss 2.23 Accuracy 0.22\n",
      "  Epoch: [2][116/191] Loss 2.45 Accuracy 0.20\n",
      "  Epoch: [2][117/191] Loss 1.86 Accuracy 0.24\n",
      "  Epoch: [2][118/191] Loss 2.05 Accuracy 0.16\n",
      "  Epoch: [2][119/191] Loss 2.21 Accuracy 0.10\n",
      "  Epoch: [2][120/191] Loss 1.90 Accuracy 0.20\n",
      "  Epoch: [2][121/191] Loss 2.00 Accuracy 0.12\n",
      "  Epoch: [2][122/191] Loss 1.97 Accuracy 0.20\n",
      "  Epoch: [2][123/191] Loss 2.31 Accuracy 0.10\n",
      "  Epoch: [2][124/191] Loss 2.06 Accuracy 0.28\n",
      "  Epoch: [2][125/191] Loss 2.08 Accuracy 0.18\n",
      "  Epoch: [2][126/191] Loss 1.99 Accuracy 0.32\n",
      "  Epoch: [2][127/191] Loss 2.11 Accuracy 0.18\n",
      "  Epoch: [2][128/191] Loss 1.82 Accuracy 0.16\n",
      "  Epoch: [2][129/191] Loss 2.38 Accuracy 0.20\n",
      "  Epoch: [2][130/191] Loss 2.46 Accuracy 0.10\n",
      "  Epoch: [2][131/191] Loss 1.97 Accuracy 0.18\n",
      "  Epoch: [2][132/191] Loss 2.09 Accuracy 0.16\n",
      "  Epoch: [2][133/191] Loss 2.10 Accuracy 0.14\n",
      "  Epoch: [2][134/191] Loss 2.43 Accuracy 0.26\n",
      "  Epoch: [2][135/191] Loss 2.18 Accuracy 0.14\n",
      "  Epoch: [2][136/191] Loss 2.20 Accuracy 0.10\n",
      "  Epoch: [2][137/191] Loss 2.37 Accuracy 0.18\n",
      "  Epoch: [2][138/191] Loss 2.17 Accuracy 0.16\n",
      "  Epoch: [2][139/191] Loss 2.14 Accuracy 0.10\n",
      "  Epoch: [2][140/191] Loss 1.87 Accuracy 0.14\n",
      "  Epoch: [2][141/191] Loss 1.72 Accuracy 0.24\n",
      "  Epoch: [2][142/191] Loss 2.47 Accuracy 0.20\n",
      "  Epoch: [2][143/191] Loss 2.29 Accuracy 0.16\n",
      "  Epoch: [2][144/191] Loss 2.41 Accuracy 0.18\n",
      "  Epoch: [2][145/191] Loss 2.14 Accuracy 0.16\n",
      "  Epoch: [2][146/191] Loss 1.94 Accuracy 0.30\n",
      "  Epoch: [2][147/191] Loss 2.34 Accuracy 0.16\n",
      "  Epoch: [2][148/191] Loss 2.36 Accuracy 0.16\n",
      "  Epoch: [2][149/191] Loss 1.87 Accuracy 0.16\n",
      "  Epoch: [2][150/191] Loss 2.23 Accuracy 0.18\n",
      "  Epoch: [2][151/191] Loss 2.78 Accuracy 0.14\n",
      "  Epoch: [2][152/191] Loss 2.23 Accuracy 0.20\n",
      "  Epoch: [2][153/191] Loss 2.31 Accuracy 0.10\n",
      "  Epoch: [2][154/191] Loss 1.95 Accuracy 0.22\n",
      "  Epoch: [2][155/191] Loss 2.01 Accuracy 0.12\n",
      "  Epoch: [2][156/191] Loss 2.16 Accuracy 0.06\n",
      "  Epoch: [2][157/191] Loss 2.08 Accuracy 0.14\n",
      "  Epoch: [2][158/191] Loss 1.93 Accuracy 0.24\n",
      "  Epoch: [2][159/191] Loss 2.41 Accuracy 0.20\n",
      "  Epoch: [2][160/191] Loss 1.95 Accuracy 0.26\n",
      "  Epoch: [2][161/191] Loss 1.64 Accuracy 0.16\n",
      "  Epoch: [2][162/191] Loss 2.50 Accuracy 0.08\n",
      "  Epoch: [2][163/191] Loss 2.22 Accuracy 0.20\n",
      "  Epoch: [2][164/191] Loss 2.36 Accuracy 0.14\n",
      "  Epoch: [2][165/191] Loss 2.23 Accuracy 0.20\n",
      "  Epoch: [2][166/191] Loss 2.19 Accuracy 0.12\n",
      "  Epoch: [2][167/191] Loss 2.23 Accuracy 0.20\n",
      "  Epoch: [2][168/191] Loss 2.26 Accuracy 0.22\n",
      "  Epoch: [2][169/191] Loss 2.34 Accuracy 0.16\n",
      "  Epoch: [2][170/191] Loss 2.41 Accuracy 0.20\n",
      "  Epoch: [2][171/191] Loss 2.10 Accuracy 0.26\n",
      "  Epoch: [2][172/191] Loss 2.05 Accuracy 0.12\n",
      "  Epoch: [2][173/191] Loss 2.17 Accuracy 0.06\n",
      "  Epoch: [2][174/191] Loss 2.44 Accuracy 0.18\n",
      "  Epoch: [2][175/191] Loss 2.04 Accuracy 0.14\n",
      "  Epoch: [2][176/191] Loss 2.63 Accuracy 0.24\n",
      "  Epoch: [2][177/191] Loss 2.09 Accuracy 0.32\n",
      "  Epoch: [2][178/191] Loss 2.30 Accuracy 0.24\n",
      "  Epoch: [2][179/191] Loss 2.12 Accuracy 0.26\n",
      "  Epoch: [2][180/191] Loss 2.09 Accuracy 0.16\n",
      "  Epoch: [2][181/191] Loss 1.89 Accuracy 0.06\n",
      "  Epoch: [2][182/191] Loss 2.40 Accuracy 0.14\n",
      "  Epoch: [2][183/191] Loss 1.83 Accuracy 0.16\n",
      "  Epoch: [2][184/191] Loss 2.21 Accuracy 0.22\n",
      "  Epoch: [2][185/191] Loss 1.94 Accuracy 0.16\n",
      "  Epoch: [2][186/191] Loss 1.76 Accuracy 0.08\n",
      "  Epoch: [2][187/191] Loss 2.24 Accuracy 0.26\n",
      "  Epoch: [2][188/191] Loss 2.17 Accuracy 0.12\n",
      "  Epoch: [2][189/191] Loss 2.03 Accuracy 0.22\n",
      "  Epoch: [2][190/191] Loss 2.23 Accuracy 0.28\n",
      "  Epoch 2: Loss 1.37 Accuracy 0.46\n",
      "  Epoch: [2][0/21] Accuracy 0.78\n",
      "  Epoch: [2][1/21] Accuracy 0.78\n",
      "  Epoch: [2][2/21] Accuracy 0.76\n",
      "  Epoch: [2][3/21] Accuracy 0.90\n",
      "  Epoch: [2][4/21] Accuracy 0.84\n",
      "  Epoch: [2][5/21] Accuracy 0.88\n",
      "  Epoch: [2][6/21] Accuracy 0.86\n",
      "  Epoch: [2][7/21] Accuracy 0.78\n",
      "  Epoch: [2][8/21] Accuracy 0.72\n",
      "  Epoch: [2][9/21] Accuracy 0.88\n",
      "  Epoch: [2][10/21] Accuracy 0.82\n",
      "  Epoch: [2][11/21] Accuracy 0.84\n",
      "  Epoch: [2][12/21] Accuracy 0.70\n",
      "  Epoch: [2][13/21] Accuracy 0.70\n",
      "  Epoch: [2][14/21] Accuracy 0.80\n",
      "  Epoch: [2][15/21] Accuracy 0.82\n",
      "  Epoch: [2][16/21] Accuracy 0.88\n",
      "  Epoch: [2][17/21] Accuracy 0.74\n",
      "  Epoch: [2][18/21] Accuracy 0.88\n",
      "  Epoch: [2][19/21] Accuracy 0.84\n",
      "  Epoch: [2][20/21] Accuracy 0.82\n",
      "Finished Epoch 2 Accuracy 0.81\n",
      "Training epoch 3\n",
      "  Epoch: [3][0/191] Loss 0.39 Accuracy 0.70\n",
      "  Epoch: [3][1/191] Loss 0.44 Accuracy 0.82\n",
      "  Epoch: [3][2/191] Loss 0.28 Accuracy 0.82\n",
      "  Epoch: [3][3/191] Loss 0.42 Accuracy 0.84\n",
      "  Epoch: [3][4/191] Loss 0.43 Accuracy 0.76\n",
      "  Epoch: [3][5/191] Loss 0.33 Accuracy 0.80\n",
      "  Epoch: [3][6/191] Loss 0.29 Accuracy 0.84\n",
      "  Epoch: [3][7/191] Loss 0.35 Accuracy 0.84\n",
      "  Epoch: [3][8/191] Loss 0.36 Accuracy 0.84\n",
      "  Epoch: [3][9/191] Loss 0.36 Accuracy 0.74\n",
      "  Epoch: [3][10/191] Loss 0.32 Accuracy 0.80\n",
      "  Epoch: [3][11/191] Loss 0.27 Accuracy 0.74\n",
      "  Epoch: [3][12/191] Loss 0.56 Accuracy 0.64\n",
      "  Epoch: [3][13/191] Loss 0.46 Accuracy 0.80\n",
      "  Epoch: [3][14/191] Loss 0.34 Accuracy 0.74\n",
      "  Epoch: [3][15/191] Loss 0.45 Accuracy 0.80\n",
      "  Epoch: [3][16/191] Loss 0.46 Accuracy 0.84\n",
      "  Epoch: [3][17/191] Loss 0.29 Accuracy 0.90\n",
      "  Epoch: [3][18/191] Loss 0.45 Accuracy 0.86\n",
      "  Epoch: [3][19/191] Loss 0.38 Accuracy 0.82\n",
      "  Epoch: [3][20/191] Loss 0.37 Accuracy 0.92\n",
      "  Epoch: [3][21/191] Loss 0.30 Accuracy 0.90\n",
      "  Epoch: [3][22/191] Loss 0.37 Accuracy 0.96\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "k_fold = 10\n",
    "for i in range(k_fold):\n",
    "    train_loss, train_acc, test_acc = [], [], []\n",
    "    n_epochs = 20\n",
    "    temp = make_train_test_split(X, i, k_fold)\n",
    "    print(\"Dataset loaded!\")\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        print(\"Training epoch \" + str(epoch))\n",
    "        train_size = ceil(len(temp[\"train\"][\"data\"])/batch_size)\n",
    "        total_loss, total_acc = 0, 0\n",
    "        for n, sample in get_batch(temp[\"train\"], batch_size):\n",
    "            train_step.run(feed_dict={input_x:sample[\"data\"], input_y:sample[\"label\"]})\n",
    "            loss = cross_entropy.eval(feed_dict={input_x:sample[\"data\"], input_y:sample[\"label\"]})\n",
    "            acc = accuracy.eval(feed_dict={input_x:sample[\"data\"], input_y:sample[\"label\"]})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "            print(\"  Epoch: [%d][%d/%d] Loss %.2f Accuracy %.2f\" % (epoch, n, train_size, loss, acc))\n",
    "        print(\"  Epoch %d: Loss %.2f Accuracy %.2f\" % (epoch, total_loss / train_size, total_acc / train_size ))\n",
    "        train_loss.append(total_loss / train_size)\n",
    "        train_acc.append(total_acc / train_size)\n",
    "        total_loss, total_acc = 0, 0\n",
    "        test_size = ceil(len(temp[\"test\"][\"data\"])/batch_size)\n",
    "        for n, sample in get_batch(temp[\"test\"], batch_size):\n",
    "            acc = accuracy.eval(feed_dict={input_x:sample[\"data\"], input_y:sample[\"label\"]})\n",
    "            total_acc += acc\n",
    "            print(\"  Epoch: [%d][%d/%d] Accuracy %.2f\" % (epoch, n, test_size, acc))\n",
    "        test_acc.append(total_acc / test_size)\n",
    "        print(\"Finished Epoch %d Accuracy %.2f\" % (epoch, total_acc / test_size))\n",
    "    print(\"=============================\")\n",
    "    results.append(max(test_acc) / n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
